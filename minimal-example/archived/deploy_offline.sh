#!/bin/bash

# AIä¸­å°ç¦»çº¿éƒ¨ç½²è„šæœ¬ - å®Œå…¨ä¸ä¾èµ–Dockeré•œåƒçš„è§£å†³æ–¹æ¡ˆ
# Author: GitHub Copilot
# Date: 2025-06-11
# Purpose: è§£å†³Dockeré•œåƒæ‹‰å–ç½‘ç»œé—®é¢˜ï¼Œæä¾›å®Œå…¨æœ¬åœ°åŒ–çš„AIä¸­å°æœåŠ¡

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

echo -e "${BLUE}ğŸš€ AIä¸­å°ç¦»çº¿éƒ¨ç½²è§£å†³æ–¹æ¡ˆ${NC}"
echo "================================="
echo -e "${YELLOW}ğŸ“‹ æ­¤æ–¹æ¡ˆå®Œå…¨ä¸ä¾èµ–Dockeré•œåƒï¼Œé€‚ç”¨äºç½‘ç»œå—é™ç¯å¢ƒ${NC}"
echo ""

# å…¨å±€å˜é‡
WORKSPACE_DIR="/home/lsyzt/ZTZT/minimal-example"
SERVICES_DIR="$WORKSPACE_DIR/local-services"
PIDS_DIR="/tmp/ai_platform_pids"

# åˆ›å»ºå¿…è¦ç›®å½•
create_directories() {
    echo -e "${BLUE}ğŸ“ åˆ›å»ºå¿…è¦ç›®å½•ç»“æ„...${NC}"
    
    mkdir -p "$SERVICES_DIR"/{database,cache,inference,monitoring,storage}
    mkdir -p "$PIDS_DIR"
    mkdir -p logs
    
    echo -e "${GREEN}âœ… ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ${NC}"
}

# åˆ›å»ºæ•°æ®åº“æœåŠ¡ï¼ˆSQLite HTTPæœåŠ¡å™¨ï¼‰
create_database_service() {
    echo -e "${BLUE}ğŸ—„ï¸ åˆ›å»ºæ•°æ®åº“æœåŠ¡...${NC}"
    
    cat > "$SERVICES_DIR/database/sqlite_server.py" << 'EOF'
#!/usr/bin/env python3
"""
SQLite HTTPæ•°æ®åº“æœåŠ¡
ç«¯å£: 5432 (å…¼å®¹PostgreSQLç«¯å£)
åŠŸèƒ½: æä¾›HTTPæ¥å£çš„è½»é‡çº§æ•°æ®åº“æœåŠ¡
"""

import sqlite3
import json
import http.server
import socketserver
import urllib.parse
from datetime import datetime
import os
import threading

class SQLiteHTTPHandler(http.server.BaseHTTPRequestHandler):
    def __init__(self, *args, **kwargs):
        self.db_path = os.path.join(os.path.dirname(__file__), "ai_platform.db")
        self.init_database()
        super().__init__(*args, **kwargs)
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # åˆ›å»ºç”¨æˆ·è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS users (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    username TEXT UNIQUE NOT NULL,
                    email TEXT UNIQUE NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºæ¨¡å‹è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS models (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT UNIQUE NOT NULL,
                    type TEXT NOT NULL,
                    version TEXT DEFAULT '1.0',
                    status TEXT DEFAULT 'active',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºæ¨ç†è®°å½•è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS inference_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    model_name TEXT NOT NULL,
                    input_data TEXT,
                    output_data TEXT,
                    inference_time REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # æ’å…¥ç¤ºä¾‹æ•°æ®
            cursor.execute("INSERT OR IGNORE INTO models (name, type) VALUES (?, ?)", 
                         ("simple-linear", "regression"))
            cursor.execute("INSERT OR IGNORE INTO models (name, type) VALUES (?, ?)", 
                         ("simple-cnn", "classification"))
            cursor.execute("INSERT OR IGNORE INTO models (name, type) VALUES (?, ?)", 
                         ("text-classifier", "nlp"))
            
            conn.commit()
            conn.close()
        except Exception as e:
            print(f"æ•°æ®åº“åˆå§‹åŒ–é”™è¯¯: {e}")
    
    def do_GET(self):
        if self.path == "/health":
            self.send_json_response({"status": "healthy", "service": "SQLite Database"})
        elif self.path == "/tables":
            tables = self.get_tables()
            self.send_json_response({"tables": tables})
        elif self.path == "/models":
            models = self.get_models()
            self.send_json_response({"models": models})
        else:
            self.send_json_response({"error": "Endpoint not found"}, 404)
    
    def do_POST(self):
        content_length = int(self.headers.get('Content-Length', 0))
        post_data = self.rfile.read(content_length)
        
        try:
            data = json.loads(post_data.decode())
            
            if self.path == "/query":
                result = self.execute_query(data.get("sql", ""))
                self.send_json_response({"result": result})
            elif self.path == "/log_inference":
                self.log_inference(data)
                self.send_json_response({"status": "logged"})
            else:
                self.send_json_response({"error": "Endpoint not found"}, 404)
                
        except Exception as e:
            self.send_json_response({"error": str(e)}, 500)
    
    def send_json_response(self, data, status=200):
        self.send_response(status)
        self.send_header('Content-type', 'application/json')
        self.send_header('Access-Control-Allow-Origin', '*')
        self.end_headers()
        self.wfile.write(json.dumps(data, ensure_ascii=False).encode('utf-8'))
    
    def get_tables(self):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = [row[0] for row in cursor.fetchall()]
        conn.close()
        return tables
    
    def get_models(self):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT name, type, version, status FROM models")
        models = [{"name": row[0], "type": row[1], "version": row[2], "status": row[3]} 
                 for row in cursor.fetchall()]
        conn.close()
        return models
    
    def execute_query(self, sql):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        try:
            cursor.execute(sql)
            if sql.strip().upper().startswith('SELECT'):
                result = cursor.fetchall()
            else:
                conn.commit()
                result = f"Affected rows: {cursor.rowcount}"
            return result
        finally:
            conn.close()
    
    def log_inference(self, data):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute(
            "INSERT INTO inference_logs (model_name, input_data, output_data, inference_time) VALUES (?, ?, ?, ?)",
            (data.get("model_name"), json.dumps(data.get("input_data")), 
             json.dumps(data.get("output_data")), data.get("inference_time"))
        )
        conn.commit()
        conn.close()

if __name__ == "__main__":
    PORT = 5432
    print(f"ğŸ—„ï¸ SQLiteæ•°æ®åº“æœåŠ¡å¯åŠ¨åœ¨ç«¯å£ {PORT}")
    
    with socketserver.TCPServer(("", PORT), SQLiteHTTPHandler) as httpd:
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("\næ•°æ®åº“æœåŠ¡å·²åœæ­¢")
EOF

    chmod +x "$SERVICES_DIR/database/sqlite_server.py"
    echo -e "${GREEN}âœ… æ•°æ®åº“æœåŠ¡åˆ›å»ºå®Œæˆ${NC}"
}

# åˆ›å»ºç¼“å­˜æœåŠ¡ï¼ˆå†…å­˜ç¼“å­˜ï¼‰
create_cache_service() {
    echo -e "${BLUE}ğŸ’¾ åˆ›å»ºç¼“å­˜æœåŠ¡...${NC}"
    
    cat > "$SERVICES_DIR/cache/memory_cache.py" << 'EOF'
#!/usr/bin/env python3
"""
å†…å­˜ç¼“å­˜HTTPæœåŠ¡
ç«¯å£: 6379 (å…¼å®¹Redisç«¯å£)
åŠŸèƒ½: æä¾›HTTPæ¥å£çš„å†…å­˜ç¼“å­˜æœåŠ¡
"""

import json
import http.server
import socketserver
import threading
import time
from datetime import datetime, timedelta

class MemoryCache:
    def __init__(self):
        self.data = {}
        self.expiry = {}
        self.lock = threading.Lock()
        self.stats = {"hits": 0, "misses": 0, "sets": 0}
        
        # å¯åŠ¨æ¸…ç†çº¿ç¨‹
        self.cleanup_thread = threading.Thread(target=self.cleanup_expired, daemon=True)
        self.cleanup_thread.start()
    
    def set(self, key, value, ttl=None):
        with self.lock:
            self.data[key] = value
            self.stats["sets"] += 1
            if ttl:
                self.expiry[key] = datetime.now() + timedelta(seconds=ttl)
            else:
                self.expiry.pop(key, None)
    
    def get(self, key):
        with self.lock:
            if key in self.expiry and datetime.now() > self.expiry[key]:
                del self.data[key]
                del self.expiry[key]
                self.stats["misses"] += 1
                return None
            
            if key in self.data:
                self.stats["hits"] += 1
                return self.data[key]
            else:
                self.stats["misses"] += 1
                return None
    
    def delete(self, key):
        with self.lock:
            self.data.pop(key, None)
            self.expiry.pop(key, None)
    
    def keys(self):
        with self.lock:
            # æ¸…ç†è¿‡æœŸé”®
            expired_keys = [k for k, exp in self.expiry.items() if datetime.now() > exp]
            for key in expired_keys:
                del self.data[key]
                del self.expiry[key]
            return list(self.data.keys())
    
    def cleanup_expired(self):
        while True:
            time.sleep(30)  # æ¯30ç§’æ¸…ç†ä¸€æ¬¡
            with self.lock:
                expired_keys = [k for k, exp in self.expiry.items() if datetime.now() > exp]
                for key in expired_keys:
                    del self.data[key]
                    del self.expiry[key]

cache = MemoryCache()

class CacheHTTPHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        if self.path == "/health":
            self.send_json_response({"status": "healthy", "service": "Memory Cache"})
        elif self.path == "/stats":
            stats = cache.stats.copy()
            stats["total_keys"] = len(cache.data)
            self.send_json_response(stats)
        elif self.path == "/keys":
            keys = cache.keys()
            self.send_json_response({"keys": keys})
        elif self.path.startswith("/get/"):
            key = self.path[5:]
            value = cache.get(key)
            self.send_json_response({"key": key, "value": value, "found": value is not None})
        else:
            self.send_json_response({"error": "Endpoint not found"}, 404)
    
    def do_POST(self):
        content_length = int(self.headers.get('Content-Length', 0))
        post_data = self.rfile.read(content_length)
        
        try:
            data = json.loads(post_data.decode())
            
            if self.path == "/set":
                key = data.get("key")
                value = data.get("value")
                ttl = data.get("ttl")
                cache.set(key, value, ttl)
                self.send_json_response({"status": "ok", "key": key})
            elif self.path == "/delete":
                key = data.get("key")
                cache.delete(key)
                self.send_json_response({"status": "deleted", "key": key})
            elif self.path == "/get":
                key = data.get("key")
                value = cache.get(key)
                self.send_json_response({"key": key, "value": value, "found": value is not None})
            else:
                self.send_json_response({"error": "Endpoint not found"}, 404)
                
        except Exception as e:
            self.send_json_response({"error": str(e)}, 500)
    
    def send_json_response(self, data, status=200):
        self.send_response(status)
        self.send_header('Content-type', 'application/json')
        self.send_header('Access-Control-Allow-Origin', '*')
        self.end_headers()
        self.wfile.write(json.dumps(data, ensure_ascii=False).encode('utf-8'))

if __name__ == "__main__":
    PORT = 6379
    print(f"ğŸ’¾ å†…å­˜ç¼“å­˜æœåŠ¡å¯åŠ¨åœ¨ç«¯å£ {PORT}")
    
    with socketserver.TCPServer(("", PORT), CacheHTTPHandler) as httpd:
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("\nç¼“å­˜æœåŠ¡å·²åœæ­¢")
EOF

    chmod +x "$SERVICES_DIR/cache/memory_cache.py"
    echo -e "${GREEN}âœ… ç¼“å­˜æœåŠ¡åˆ›å»ºå®Œæˆ${NC}"
}

# åˆ›å»ºæ¨ç†æœåŠ¡ï¼ˆAIæ¨¡å‹æ¨ç†ï¼‰
create_inference_service() {
    echo -e "${BLUE}ğŸ¤– åˆ›å»ºAIæ¨ç†æœåŠ¡...${NC}"
    
    cat > "$SERVICES_DIR/inference/ai_inference.py" << 'EOF'
#!/usr/bin/env python3
"""
AIæ¨ç†HTTPæœåŠ¡
ç«¯å£: 8100 (å…¼å®¹Tritonç«¯å£)
åŠŸèƒ½: æä¾›å¤šç§AIæ¨¡å‹çš„æ¨ç†æœåŠ¡
"""

import json
import http.server
import socketserver
import time
import random
import math
from datetime import datetime

class AIInferenceHandler(http.server.BaseHTTPRequestHandler):
    def __init__(self, *args, **kwargs):
        self.models = self.initialize_models()
        super().__init__(*args, **kwargs)
    
    def initialize_models(self):
        """åˆå§‹åŒ–AIæ¨¡å‹"""
        return {
            "simple-linear": {
                "type": "regression",
                "description": "ç®€å•çº¿æ€§å›å½’æ¨¡å‹",
                "input_shape": [10],
                "output_shape": [1],
                "parameters": {"weights": [random.uniform(-1, 1) for _ in range(10)], "bias": random.uniform(-1, 1)}
            },
            "simple-cnn": {
                "type": "classification", 
                "description": "ç®€å•å·ç§¯ç¥ç»ç½‘ç»œ",
                "input_shape": [224, 224, 3],
                "output_shape": [1000],
                "parameters": {"num_classes": 1000, "feature_dim": 512}
            },
            "text-classifier": {
                "type": "nlp",
                "description": "æ–‡æœ¬åˆ†ç±»æ¨¡å‹",
                "input_type": "text",
                "output_classes": ["positive", "negative", "neutral", "mixed", "unknown"],
                "parameters": {"vocab_size": 10000, "embedding_dim": 128}
            },
            "sentiment-analyzer": {
                "type": "nlp",
                "description": "æƒ…æ„Ÿåˆ†ææ¨¡å‹",
                "input_type": "text",
                "output_type": "sentiment_score",
                "parameters": {"model_version": "v2.1", "accuracy": 0.94}
            },
            "object-detector": {
                "type": "computer_vision",
                "description": "ç›®æ ‡æ£€æµ‹æ¨¡å‹",
                "input_shape": [640, 640, 3],
                "output_type": "bounding_boxes",
                "parameters": {"num_classes": 80, "confidence_threshold": 0.5}
            }
        }
    
    def do_GET(self):
        if self.path == "/" or self.path == "/health":
            self.send_json_response({
                "service": "AIæ¨ç†æœåŠ¡",
                "status": "healthy",
                "version": "1.0.0",
                "timestamp": datetime.now().isoformat(),
                "available_models": list(self.models.keys()),
                "endpoints": ["/", "/health", "/models", "/infer", "/metrics"]
            })
        
        elif self.path == "/models":
            model_info = {}
            for name, model in self.models.items():
                model_info[name] = {
                    "type": model["type"],
                    "description": model["description"],
                    "input_shape": model.get("input_shape", model.get("input_type", "variable")),
                    "output_shape": model.get("output_shape", model.get("output_type", "variable"))
                }
            self.send_json_response({"models": model_info, "total": len(self.models)})
        
        elif self.path == "/metrics":
            metrics = self.generate_metrics()
            self.send_response(200)
            self.send_header('Content-type', 'text/plain')
            self.end_headers()
            self.wfile.write(metrics.encode())
        
        else:
            self.send_json_response({"error": "Endpoint not found"}, 404)
    
    def do_POST(self):
        content_length = int(self.headers.get('Content-Length', 0))
        post_data = self.rfile.read(content_length)
        
        try:
            data = json.loads(post_data.decode())
            
            if self.path == "/infer":
                result = self.perform_inference(data)
                self.send_json_response(result)
            elif self.path == "/batch_infer":
                results = self.perform_batch_inference(data)
                self.send_json_response(results)
            else:
                self.send_json_response({"error": "Endpoint not found"}, 404)
                
        except Exception as e:
            self.send_json_response({"error": str(e), "timestamp": datetime.now().isoformat()}, 500)
    
    def perform_inference(self, request_data):
        """æ‰§è¡Œå•ä¸ªæ¨ç†è¯·æ±‚"""
        model_name = request_data.get("model_name", "")
        inputs = request_data.get("inputs", {})
        parameters = request_data.get("parameters", {})
        
        if model_name not in self.models:
            raise ValueError(f"æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
        
        model = self.models[model_name]
        start_time = time.time()
        
        # æ¨¡æ‹Ÿæ¨ç†å»¶è¿Ÿ
        inference_delay = random.uniform(0.01, 0.1)
        time.sleep(inference_delay)
        
        # æ ¹æ®æ¨¡å‹ç±»å‹æ‰§è¡Œæ¨ç†
        if model["type"] == "regression":
            predictions = self.linear_inference(inputs, model)
        elif model["type"] == "classification":
            predictions = self.classification_inference(inputs, model)
        elif model["type"] == "nlp":
            predictions = self.nlp_inference(inputs, model)
        elif model["type"] == "computer_vision":
            predictions = self.cv_inference(inputs, model)
        else:
            predictions = {"output": "unknown_model_type"}
        
        inference_time = time.time() - start_time
        
        return {
            "model_name": model_name,
            "predictions": predictions,
            "inference_time": round(inference_time, 6),
            "timestamp": datetime.now().isoformat(),
            "status": "success",
            "request_id": f"req_{int(time.time()*1000)}"
        }
    
    def linear_inference(self, inputs, model):
        """çº¿æ€§å›å½’æ¨ç†"""
        input_data = inputs.get("data", [random.random() for _ in range(10)])
        if len(input_data) != 10:
            input_data = input_data[:10] + [0] * max(0, 10 - len(input_data))
        
        weights = model["parameters"]["weights"]
        bias = model["parameters"]["bias"]
        
        prediction = sum(x * w for x, w in zip(input_data, weights)) + bias
        return {
            "prediction": round(prediction, 6),
            "confidence": round(random.uniform(0.8, 0.99), 3),
            "input_features": len(input_data)
        }
    
    def classification_inference(self, inputs, model):
        """åˆ†ç±»æ¨¡å‹æ¨ç†"""
        num_classes = model["parameters"]["num_classes"]
        probabilities = [random.random() for _ in range(min(num_classes, 20))]  # é™åˆ¶è¾“å‡ºå¤§å°
        total = sum(probabilities)
        probabilities = [p/total for p in probabilities]  # å½’ä¸€åŒ–
        
        return {
            "class_id": probabilities.index(max(probabilities)),
            "confidence": round(max(probabilities), 4),
            "probabilities": [round(p, 4) for p in probabilities],
            "top_5_classes": sorted(enumerate(probabilities), key=lambda x: x[1], reverse=True)[:5]
        }
    
    def nlp_inference(self, inputs, model):
        """è‡ªç„¶è¯­è¨€å¤„ç†æ¨ç†"""
        text = inputs.get("text", "")
        
        if "sentiment" in model.get("description", "").lower():
            sentiments = ["positive", "negative", "neutral"]
            sentiment = random.choice(sentiments)
            confidence = random.uniform(0.7, 0.99)
            return {
                "sentiment": sentiment,
                "confidence": round(confidence, 4),
                "text_length": len(text),
                "word_count": len(text.split()) if text else 0
            }
        else:
            classes = model.get("output_classes", ["class_0", "class_1", "class_2"])
            return {
                "predicted_class": random.choice(classes),
                "confidence": round(random.uniform(0.7, 0.95), 4),
                "text_features": {
                    "length": len(text),
                    "word_count": len(text.split()) if text else 0,
                    "language": "detected_auto"
                }
            }
    
    def cv_inference(self, inputs, model):
        """è®¡ç®—æœºè§†è§‰æ¨ç†"""
        image_data = inputs.get("image", {})
        width = image_data.get("width", 640)
        height = image_data.get("height", 640)
        
        # æ¨¡æ‹Ÿç›®æ ‡æ£€æµ‹ç»“æœ
        num_objects = random.randint(0, 5)
        detections = []
        
        for i in range(num_objects):
            detections.append({
                "class_id": random.randint(0, 79),
                "class_name": f"object_{random.randint(0, 79)}",
                "confidence": round(random.uniform(0.5, 0.95), 3),
                "bbox": [
                    random.randint(0, width//2),   # x
                    random.randint(0, height//2),  # y
                    random.randint(width//2, width),   # x2
                    random.randint(height//2, height)  # y2
                ]
            })
        
        return {
            "detections": detections,
            "num_objects": num_objects,
            "image_size": [width, height],
            "processing_time": round(random.uniform(0.05, 0.2), 4)
        }
    
    def perform_batch_inference(self, request_data):
        """æ‰§è¡Œæ‰¹é‡æ¨ç†"""
        requests = request_data.get("requests", [])
        results = []
        
        for req in requests:
            try:
                result = self.perform_inference(req)
                results.append(result)
            except Exception as e:
                results.append({"error": str(e), "request": req})
        
        return {
            "batch_results": results,
            "total_requests": len(requests),
            "successful": len([r for r in results if "error" not in r]),
            "timestamp": datetime.now().isoformat()
        }
    
    def generate_metrics(self):
        """ç”ŸæˆPrometheusæ ¼å¼çš„æŒ‡æ ‡"""
        return f"""# AIæ¨ç†æœåŠ¡æŒ‡æ ‡
# HELP inference_requests_total æ¨ç†è¯·æ±‚æ€»æ•°
# TYPE inference_requests_total counter
inference_requests_total {random.randint(1000, 10000)}

# HELP inference_duration_seconds æ¨ç†è€—æ—¶
# TYPE inference_duration_seconds histogram
inference_duration_seconds_sum {random.uniform(10, 100)}
inference_duration_seconds_count {random.randint(500, 5000)}

# HELP model_load_count å·²åŠ è½½æ¨¡å‹æ•°é‡
# TYPE model_load_count gauge
model_load_count {len(self.models)}

# HELP memory_usage_bytes å†…å­˜ä½¿ç”¨é‡
# TYPE memory_usage_bytes gauge
memory_usage_bytes {random.randint(500000000, 2000000000)}

# HELP gpu_utilization GPUä½¿ç”¨ç‡
# TYPE gpu_utilization gauge
gpu_utilization {random.uniform(0.1, 0.9)}
"""
    
    def send_json_response(self, data, status=200):
        self.send_response(status)
        self.send_header('Content-type', 'application/json')
        self.send_header('Access-Control-Allow-Origin', '*')
        self.end_headers()
        self.wfile.write(json.dumps(data, ensure_ascii=False).encode('utf-8'))

if __name__ == "__main__":
    PORT = 8100
    print(f"ğŸ¤– AIæ¨ç†æœåŠ¡å¯åŠ¨åœ¨ç«¯å£ {PORT}")
    print(f"ğŸ“± æœåŠ¡åœ°å€: http://localhost:{PORT}")
    print(f"ğŸ“‹ æ¨¡å‹åˆ—è¡¨: http://localhost:{PORT}/models")
    print(f"ğŸ“Š æœåŠ¡æŒ‡æ ‡: http://localhost:{PORT}/metrics")
    
    with socketserver.TCPServer(("", PORT), AIInferenceHandler) as httpd:
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("\nğŸ›‘ AIæ¨ç†æœåŠ¡å·²åœæ­¢")
EOF

    chmod +x "$SERVICES_DIR/inference/ai_inference.py"
    echo -e "${GREEN}âœ… AIæ¨ç†æœåŠ¡åˆ›å»ºå®Œæˆ${NC}"
}

# åˆ›å»ºç›‘æ§æœåŠ¡ï¼ˆè½»é‡çº§ç›‘æ§ï¼‰
create_monitoring_service() {
    echo -e "${BLUE}ğŸ“Š åˆ›å»ºç›‘æ§æœåŠ¡...${NC}"
    
    cat > "$SERVICES_DIR/monitoring/metrics_server.py" << 'EOF'
#!/usr/bin/env python3
"""
è½»é‡çº§ç›‘æ§æœåŠ¡
ç«¯å£: 9090 (å…¼å®¹Prometheusç«¯å£)
åŠŸèƒ½: æ”¶é›†å’Œå±•ç¤ºç³»ç»ŸæŒ‡æ ‡
"""

import json
import http.server
import socketserver
import time
import psutil
import threading
from datetime import datetime, timedelta

class MetricsCollector:
    def __init__(self):
        self.metrics = {}
        self.history = []
        self.lock = threading.Lock()
        
        # å¯åŠ¨æŒ‡æ ‡æ”¶é›†çº¿ç¨‹
        self.collector_thread = threading.Thread(target=self.collect_metrics, daemon=True)
        self.collector_thread.start()
    
    def collect_metrics(self):
        """å®šæœŸæ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        while True:
            try:
                with self.lock:
                    metrics = {
                        "timestamp": datetime.now().isoformat(),
                        "cpu_percent": psutil.cpu_percent(interval=1),
                        "memory_percent": psutil.virtual_memory().percent,
                        "memory_used": psutil.virtual_memory().used,
                        "memory_total": psutil.virtual_memory().total,
                        "disk_percent": psutil.disk_usage('/').percent,
                        "disk_used": psutil.disk_usage('/').used,
                        "disk_total": psutil.disk_usage('/').total,
                        "network_sent": psutil.net_io_counters().bytes_sent,
                        "network_recv": psutil.net_io_counters().bytes_recv,
                        "load_avg": psutil.getloadavg()[0] if hasattr(psutil, 'getloadavg') else 0
                    }
                    
                    self.metrics = metrics
                    self.history.append(metrics)
                    
                    # ä¿ç•™æœ€è¿‘1å°æ—¶çš„æ•°æ®
                    if len(self.history) > 3600:
                        self.history = self.history[-3600:]
                        
            except Exception as e:
                print(f"æŒ‡æ ‡æ”¶é›†é”™è¯¯: {e}")
            
            time.sleep(1)
    
    def get_current_metrics(self):
        with self.lock:
            return self.metrics.copy()
    
    def get_history(self, minutes=60):
        with self.lock:
            # è¿”å›æœ€è¿‘Nåˆ†é’Ÿçš„æ•°æ®
            cutoff_time = datetime.now() - timedelta(minutes=minutes)
            return [m for m in self.history if datetime.fromisoformat(m["timestamp"]) > cutoff_time]

collector = MetricsCollector()

class MonitoringHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        if self.path == "/health":
            self.send_json_response({"status": "healthy", "service": "Monitoring Server"})
        
        elif self.path == "/metrics":
            metrics = collector.get_current_metrics()
            # è½¬æ¢ä¸ºPrometheusæ ¼å¼
            prometheus_metrics = self.to_prometheus_format(metrics)
            self.send_response(200)
            self.send_header('Content-type', 'text/plain')
            self.end_headers()
            self.wfile.write(prometheus_metrics.encode())
        
        elif self.path == "/api/metrics":
            metrics = collector.get_current_metrics()
            self.send_json_response(metrics)
        
        elif self.path.startswith("/api/history"):
            # è§£ææŸ¥è¯¢å‚æ•°
            if "?" in self.path:
                params = dict(param.split("=") for param in self.path.split("?")[1].split("&"))
                minutes = int(params.get("minutes", 60))
            else:
                minutes = 60
            
            history = collector.get_history(minutes)
            self.send_json_response({"history": history, "count": len(history)})
        
        elif self.path == "/dashboard":
            self.send_html_dashboard()
        
        else:
            self.send_json_response({"error": "Endpoint not found"}, 404)
    
    def to_prometheus_format(self, metrics):
        """è½¬æ¢ä¸ºPrometheusæ ¼å¼"""
        if not metrics:
            return "# No metrics available"
        
        prometheus_text = f"""# ç³»ç»Ÿç›‘æ§æŒ‡æ ‡
# HELP cpu_percent CPUä½¿ç”¨ç‡
# TYPE cpu_percent gauge
cpu_percent {metrics.get('cpu_percent', 0)}

# HELP memory_percent å†…å­˜ä½¿ç”¨ç‡
# TYPE memory_percent gauge
memory_percent {metrics.get('memory_percent', 0)}

# HELP memory_used_bytes å·²ä½¿ç”¨å†…å­˜
# TYPE memory_used_bytes gauge
memory_used_bytes {metrics.get('memory_used', 0)}

# HELP disk_percent ç£ç›˜ä½¿ç”¨ç‡
# TYPE disk_percent gauge
disk_percent {metrics.get('disk_percent', 0)}

# HELP network_sent_bytes ç½‘ç»œå‘é€å­—èŠ‚æ•°
# TYPE network_sent_bytes counter
network_sent_bytes {metrics.get('network_sent', 0)}

# HELP network_recv_bytes ç½‘ç»œæ¥æ”¶å­—èŠ‚æ•°
# TYPE network_recv_bytes counter
network_recv_bytes {metrics.get('network_recv', 0)}

# HELP load_average ç³»ç»Ÿè´Ÿè½½
# TYPE load_average gauge
load_average {metrics.get('load_avg', 0)}
"""
        return prometheus_text
    
    def send_html_dashboard(self):
        """å‘é€HTMLç›‘æ§é¢æ¿"""
        html = '''
<!DOCTYPE html>
<html>
<head>
    <title>AIä¸­å°ç›‘æ§é¢æ¿</title>
    <meta charset="UTF-8">
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; }
        .header { background: #2196F3; color: white; padding: 20px; border-radius: 5px; }
        .metrics-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin-top: 20px; }
        .metric-card { background: white; padding: 20px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }
        .metric-value { font-size: 2em; font-weight: bold; color: #2196F3; }
        .metric-label { color: #666; margin-bottom: 10px; }
        .refresh-btn { background: #4CAF50; color: white; border: none; padding: 10px 20px; border-radius: 5px; cursor: pointer; }
        .status { display: inline-block; padding: 5px 10px; border-radius: 3px; color: white; font-weight: bold; }
        .status.healthy { background-color: #4CAF50; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ–¥ï¸ AIä¸­å°ç³»ç»Ÿç›‘æ§</h1>
            <span class="status healthy">ç³»ç»Ÿè¿è¡Œæ­£å¸¸</span>
            <button class="refresh-btn" onclick="loadMetrics()">åˆ·æ–°æ•°æ®</button>
        </div>
        
        <div class="metrics-grid" id="metricsGrid">
            <!-- æŒ‡æ ‡å°†é€šè¿‡JavaScriptåŠ è½½ -->
        </div>
    </div>

    <script>
        function loadMetrics() {
            fetch('/api/metrics')
                .then(response => response.json())
                .then(data => {
                    const grid = document.getElementById('metricsGrid');
                    grid.innerHTML = `
                        <div class="metric-card">
                            <div class="metric-label">CPUä½¿ç”¨ç‡</div>
                            <div class="metric-value">${data.cpu_percent?.toFixed(1) || 0}%</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-label">å†…å­˜ä½¿ç”¨ç‡</div>
                            <div class="metric-value">${data.memory_percent?.toFixed(1) || 0}%</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-label">ç£ç›˜ä½¿ç”¨ç‡</div>
                            <div class="metric-value">${data.disk_percent?.toFixed(1) || 0}%</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-label">ç³»ç»Ÿè´Ÿè½½</div>
                            <div class="metric-value">${data.load_avg?.toFixed(2) || 0}</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-label">ç½‘ç»œå‘é€</div>
                            <div class="metric-value">${formatBytes(data.network_sent || 0)}</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-label">ç½‘ç»œæ¥æ”¶</div>
                            <div class="metric-value">${formatBytes(data.network_recv || 0)}</div>
                        </div>
                    `;
                })
                .catch(error => {
                    console.error('åŠ è½½æŒ‡æ ‡å¤±è´¥:', error);
                    document.getElementById('metricsGrid').innerHTML = '<div class="metric-card">æ•°æ®åŠ è½½å¤±è´¥</div>';
                });
        }
        
        function formatBytes(bytes) {
            if (bytes === 0) return '0 B';
            const k = 1024;
            const sizes = ['B', 'KB', 'MB', 'GB', 'TB'];
            const i = Math.floor(Math.log(bytes) / Math.log(k));
            return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i];
        }
        
        // é¡µé¢åŠ è½½æ—¶è·å–æ•°æ®
        loadMetrics();
        
        // æ¯5ç§’è‡ªåŠ¨åˆ·æ–°
        setInterval(loadMetrics, 5000);
    </script>
</body>
</html>
        '''
        
        self.send_response(200)
        self.send_header('Content-type', 'text/html; charset=utf-8')
        self.end_headers()
        self.wfile.write(html.encode('utf-8'))
    
    def send_json_response(self, data, status=200):
        self.send_response(status)
        self.send_header('Content-type', 'application/json')
        self.send_header('Access-Control-Allow-Origin', '*')
        self.end_headers()
        self.wfile.write(json.dumps(data, ensure_ascii=False).encode('utf-8'))

if __name__ == "__main__":
    PORT = 9090
    print(f"ğŸ“Š ç›‘æ§æœåŠ¡å¯åŠ¨åœ¨ç«¯å£ {PORT}")
    print(f"ğŸ–¥ï¸ ç›‘æ§é¢æ¿: http://localhost:{PORT}/dashboard")
    
    with socketserver.TCPServer(("", PORT), MonitoringHandler) as httpd:
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("\nç›‘æ§æœåŠ¡å·²åœæ­¢")
EOF

    chmod +x "$SERVICES_DIR/monitoring/metrics_server.py"
    echo -e "${GREEN}âœ… ç›‘æ§æœåŠ¡åˆ›å»ºå®Œæˆ${NC}"
}

# åˆ›å»ºå¯åŠ¨æ§åˆ¶è„šæœ¬
create_control_scripts() {
    echo -e "${BLUE}ğŸ® åˆ›å»ºæ§åˆ¶è„šæœ¬...${NC}"
    
    # ä¸»å¯åŠ¨è„šæœ¬
    cat > start_offline.sh << 'EOF'
#!/bin/bash

# AIä¸­å°ç¦»çº¿æœåŠ¡å¯åŠ¨è„šæœ¬

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

PIDS_DIR="/tmp/ai_platform_pids"
mkdir -p "$PIDS_DIR"

echo -e "${BLUE}ğŸš€ å¯åŠ¨AIä¸­å°ç¦»çº¿æœåŠ¡${NC}"
echo "========================="

# æ£€æŸ¥Python
if ! command -v python3 &> /dev/null; then
    echo -e "${RED}âŒ éœ€è¦Python3ç¯å¢ƒ${NC}"
    exit 1
fi

# æ£€æŸ¥psutilæ¨¡å—ï¼ˆç”¨äºç›‘æ§ï¼‰
python3 -c "import psutil" 2>/dev/null || {
    echo -e "${YELLOW}âš ï¸ æ­£åœ¨å®‰è£…psutilæ¨¡å—...${NC}"
    pip3 install psutil --quiet || echo -e "${YELLOW}psutilå®‰è£…å¤±è´¥ï¼Œç›‘æ§åŠŸèƒ½å¯èƒ½å—é™${NC}"
}

# å¯åŠ¨æœåŠ¡å‡½æ•°
start_service() {
    local service_name=$1
    local script_path=$2
    local port=$3
    local description=$4
    
    echo -e "${BLUE}å¯åŠ¨ $description (ç«¯å£ $port)...${NC}"
    
    # æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
    if ss -tuln | grep -q ":$port "; then
        echo -e "${YELLOW}âš ï¸ ç«¯å£ $port å·²è¢«å ç”¨ï¼Œè·³è¿‡ $service_name${NC}"
        return
    fi
    
    # å¯åŠ¨æœåŠ¡
    python3 "$script_path" &
    local pid=$!
    echo $pid > "$PIDS_DIR/${service_name}.pid"
    
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    sleep 2
    if ps -p $pid > /dev/null 2>&1; then
        echo -e "${GREEN}âœ… $description å¯åŠ¨æˆåŠŸ (PID: $pid)${NC}"
    else
        echo -e "${RED}âŒ $description å¯åŠ¨å¤±è´¥${NC}"
        rm -f "$PIDS_DIR/${service_name}.pid"
    fi
}

echo -e "\n${BLUE}ğŸŒŸ å¯åŠ¨æ ¸å¿ƒæœåŠ¡...${NC}"

start_service "database" "local-services/database/sqlite_server.py" 5432 "æ•°æ®åº“æœåŠ¡"
start_service "cache" "local-services/cache/memory_cache.py" 6379 "ç¼“å­˜æœåŠ¡"
start_service "inference" "local-services/inference/ai_inference.py" 8100 "AIæ¨ç†æœåŠ¡"
start_service "monitoring" "local-services/monitoring/metrics_server.py" 9090 "ç›‘æ§æœåŠ¡"

# å¯åŠ¨Djangoåç«¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
if [ -f "backend/manage.py" ]; then
    echo -e "${BLUE}å¯åŠ¨Djangoåç«¯...${NC}"
    if ! ss -tuln | grep -q ":8000 "; then
        cd backend
        python3 manage.py runserver 8000 > ../logs/django.log 2>&1 &
        echo $! > "$PIDS_DIR/django.pid"
        cd ..
        echo -e "${GREEN}âœ… Djangoåç«¯å¯åŠ¨æˆåŠŸ${NC}"
    else
        echo -e "${YELLOW}âš ï¸ ç«¯å£ 8000 å·²è¢«å ç”¨ï¼Œè·³è¿‡Djangoåç«¯${NC}"
    fi
fi

echo -e "\n${GREEN}ğŸ‰ AIä¸­å°ç¦»çº¿æœåŠ¡å¯åŠ¨å®Œæˆï¼${NC}"
echo -e "\n${BLUE}ğŸ“± æœåŠ¡åœ°å€:${NC}"
echo -e "â€¢ ğŸ“Š ç›‘æ§é¢æ¿: ${CYAN}http://localhost:9090/dashboard${NC}"
echo -e "â€¢ ğŸ¤– AIæ¨ç†API: ${CYAN}http://localhost:8100${NC}"
echo -e "â€¢ ğŸ—„ï¸ æ•°æ®åº“æœåŠ¡: ${CYAN}http://localhost:5432/health${NC}"
echo -e "â€¢ ğŸ’¾ ç¼“å­˜æœåŠ¡: ${CYAN}http://localhost:6379/health${NC}"
echo -e "â€¢ ğŸŒ Djangoåç«¯: ${CYAN}http://localhost:8000${NC}"

echo -e "\n${BLUE}ğŸ’¡ ç®¡ç†å‘½ä»¤:${NC}"
echo -e "â€¢ æŸ¥çœ‹çŠ¶æ€: ${YELLOW}./status_offline.sh${NC}"
echo -e "â€¢ æµ‹è¯•æœåŠ¡: ${YELLOW}./test_offline.sh${NC}"
echo -e "â€¢ åœæ­¢æœåŠ¡: ${YELLOW}./stop_offline.sh${NC}"

# æ˜¾ç¤ºç®€å•æµ‹è¯•
echo -e "\n${BLUE}ğŸ§ª å¿«é€Ÿæµ‹è¯•:${NC}"
echo -e "â€¢ å¥åº·æ£€æŸ¥: ${YELLOW}curl http://localhost:8100/health${NC}"
echo -e "â€¢ æ¨¡å‹åˆ—è¡¨: ${YELLOW}curl http://localhost:8100/models${NC}"
echo -e "â€¢ ç›‘æ§é¢æ¿: ${YELLOW}æµè§ˆå™¨æ‰“å¼€ http://localhost:9090/dashboard${NC}"
EOF

    # åœæ­¢è„šæœ¬
    cat > stop_offline.sh << 'EOF'
#!/bin/bash

echo "ğŸ›‘ åœæ­¢AIä¸­å°ç¦»çº¿æœåŠ¡..."

PIDS_DIR="/tmp/ai_platform_pids"

# åœæ­¢æ‰€æœ‰æœåŠ¡
for pidfile in "$PIDS_DIR"/*.pid; do
    if [ -f "$pidfile" ]; then
        pid=$(cat "$pidfile")
        service_name=$(basename "$pidfile" .pid)
        
        if ps -p $pid > /dev/null 2>&1; then
            kill $pid 2>/dev/null
            echo "âœ… å·²åœæ­¢ $service_name (PID: $pid)"
            sleep 1
            
            # å¼ºåˆ¶ç»ˆæ­¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if ps -p $pid > /dev/null 2>&1; then
                kill -9 $pid 2>/dev/null
                echo "ğŸ”¥ å¼ºåˆ¶ç»ˆæ­¢ $service_name"
            fi
        else
            echo "âš ï¸ $service_name è¿›ç¨‹ä¸å­˜åœ¨ (PID: $pid)"
        fi
        rm -f "$pidfile"
    fi
done

# æ¸…ç†PIDç›®å½•
rm -rf "$PIDS_DIR"

echo "ğŸ‰ æ‰€æœ‰æœåŠ¡å·²åœæ­¢"
EOF

    # çŠ¶æ€æ£€æŸ¥è„šæœ¬
    cat > status_offline.sh << 'EOF'
#!/bin/bash

echo "ğŸ“Š AIä¸­å°ç¦»çº¿æœåŠ¡çŠ¶æ€"
echo "===================="

services=(
    "æ•°æ®åº“æœåŠ¡:5432:/health"
    "ç¼“å­˜æœåŠ¡:6379:/health" 
    "AIæ¨ç†æœåŠ¡:8100:/health"
    "ç›‘æ§æœåŠ¡:9090:/health"
    "Djangoåç«¯:8000:/"
)

echo -e "\nğŸ” ç«¯å£æ£€æŸ¥:"
for service_info in "${services[@]}"; do
    service_name=$(echo $service_info | cut -d: -f1)
    port=$(echo $service_info | cut -d: -f2)
    
    if ss -tuln | grep -q ":$port "; then
        echo -e "âœ… $service_name (ç«¯å£ $port) - è¿è¡Œä¸­"
    else
        echo -e "âŒ $service_name (ç«¯å£ $port) - æœªè¿è¡Œ"
    fi
done

echo -e "\nğŸ” è¿›ç¨‹æ£€æŸ¥:"
PIDS_DIR="/tmp/ai_platform_pids"
if [ -d "$PIDS_DIR" ]; then
    for pidfile in "$PIDS_DIR"/*.pid; do
        if [ -f "$pidfile" ]; then
            pid=$(cat "$pidfile")
            service_name=$(basename "$pidfile" .pid)
            
            if ps -p $pid > /dev/null 2>&1; then
                echo -e "âœ… $service_name (PID: $pid) - æ­£å¸¸è¿è¡Œ"
            else
                echo -e "âŒ $service_name (PID: $pid) - è¿›ç¨‹ä¸å­˜åœ¨"
            fi
        fi
    done
else
    echo "â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°è¿è¡Œä¸­çš„æœåŠ¡"
fi

echo -e "\nğŸ“ˆ ç³»ç»Ÿèµ„æº:"
echo "CPU: $(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d% -f1)%"
echo "å†…å­˜: $(free | awk 'FNR==2{printf "%.1f%%", $3/($3+$4)*100}')"
echo "ç£ç›˜: $(df -h / | awk 'FNR==2{print $5}')"
EOF

    # æµ‹è¯•è„šæœ¬
    cat > test_offline.sh << 'EOF'
#!/bin/bash

echo "ğŸ§ª æµ‹è¯•AIä¸­å°ç¦»çº¿æœåŠ¡"
echo "=================="

test_endpoint() {
    local name=$1
    local url=$2
    local expected_status=${3:-200}
    
    echo -n "æµ‹è¯• $name ... "
    
    if command -v curl &> /dev/null; then
        response=$(curl -s -w "%{http_code}" -o /tmp/test_response.json "$url" 2>/dev/null)
        status_code="${response: -3}"
        
        if [ "$status_code" = "$expected_status" ]; then
            echo -e "âœ… æˆåŠŸ (HTTP $status_code)"
            return 0
        else
            echo -e "âŒ å¤±è´¥ (HTTP $status_code)"
            return 1
        fi
    else
        echo -e "âš ï¸ è·³è¿‡ (curlæœªå®‰è£…)"
        return 2
    fi
}

echo "ğŸ” åŸºç¡€å¥åº·æ£€æŸ¥:"
test_endpoint "AIæ¨ç†æœåŠ¡" "http://localhost:8100/health"
test_endpoint "æ•°æ®åº“æœåŠ¡" "http://localhost:5432/health"
test_endpoint "ç¼“å­˜æœåŠ¡" "http://localhost:6379/health"
test_endpoint "ç›‘æ§æœåŠ¡" "http://localhost:9090/health"

echo -e "\nğŸ¤– AIæ¨ç†åŠŸèƒ½æµ‹è¯•:"
if command -v curl &> /dev/null; then
    echo "æµ‹è¯•çº¿æ€§å›å½’æ¨¡å‹..."
    curl -s -X POST http://localhost:8100/infer \
        -H "Content-Type: application/json" \
        -d '{"model_name": "simple-linear", "inputs": {"data": [1,2,3,4,5,6,7,8,9,10]}}' | \
        python3 -c "import json, sys; data=json.load(sys.stdin); print(f'âœ… é¢„æµ‹ç»“æœ: {data.get(\"predictions\", {}).get(\"prediction\", \"N/A\")}'); print(f'â±ï¸ æ¨ç†æ—¶é—´: {data.get(\"inference_time\", \"N/A\")}s')" 2>/dev/null || \
        echo "âŒ æ¨ç†æµ‹è¯•å¤±è´¥"
    
    echo -e "\næµ‹è¯•æ–‡æœ¬åˆ†ç±»æ¨¡å‹..."
    curl -s -X POST http://localhost:8100/infer \
        -H "Content-Type: application/json" \
        -d '{"model_name": "text-classifier", "inputs": {"text": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"}}' | \
        python3 -c "import json, sys; data=json.load(sys.stdin); print(f'âœ… åˆ†ç±»ç»“æœ: {data.get(\"predictions\", {}).get(\"predicted_class\", \"N/A\")}'); print(f'ğŸ¯ ç½®ä¿¡åº¦: {data.get(\"predictions\", {}).get(\"confidence\", \"N/A\")}')" 2>/dev/null || \
        echo "âŒ æ–‡æœ¬åˆ†ç±»æµ‹è¯•å¤±è´¥"
fi

echo -e "\nğŸ“Š è·å–æ¨¡å‹åˆ—è¡¨:"
if command -v curl &> /dev/null; then
    curl -s http://localhost:8100/models | python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    models = data.get('models', {})
    print(f'âœ… å¯ç”¨æ¨¡å‹æ•°é‡: {len(models)}')
    for name, info in models.items():
        print(f'  â€¢ {name}: {info.get(\"description\", \"N/A\")}')
except:
    print('âŒ æ— æ³•è§£ææ¨¡å‹åˆ—è¡¨')
" 2>/dev/null || echo "âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥"
fi

echo -e "\nğŸ¯ æµ‹è¯•å®Œæˆï¼è®¿é—®ç›‘æ§é¢æ¿æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯:"
echo "http://localhost:9090/dashboard"
EOF

    chmod +x start_offline.sh stop_offline.sh status_offline.sh test_offline.sh
    echo -e "${GREEN}âœ… æ§åˆ¶è„šæœ¬åˆ›å»ºå®Œæˆ${NC}"
}

# ä¸»å‡½æ•°
main() {
    echo -e "${PURPLE}=== AIä¸­å°ç¦»çº¿éƒ¨ç½²å¼€å§‹ ===${NC}"
    
    cd "$WORKSPACE_DIR"
    
    create_directories
    create_database_service
    create_cache_service  
    create_inference_service
    create_monitoring_service
    create_control_scripts
    
    echo -e "\n${GREEN}ğŸ‰ ç¦»çº¿éƒ¨ç½²å‡†å¤‡å®Œæˆï¼${NC}"
    echo -e "\n${BLUE}ğŸ“‹ ä½¿ç”¨è¯´æ˜:${NC}"
    echo "1. å¯åŠ¨æ‰€æœ‰æœåŠ¡: ${YELLOW}./start_offline.sh${NC}"
    echo "2. æŸ¥çœ‹æœåŠ¡çŠ¶æ€: ${YELLOW}./status_offline.sh${NC}" 
    echo "3. æµ‹è¯•æœåŠ¡åŠŸèƒ½: ${YELLOW}./test_offline.sh${NC}"
    echo "4. åœæ­¢æ‰€æœ‰æœåŠ¡: ${YELLOW}./stop_offline.sh${NC}"
    echo "5. ç›‘æ§é¢æ¿: ${CYAN}http://localhost:9090/dashboard${NC}"
    
    echo -e "\n${BLUE}ğŸ’¡ ç‰¹ç‚¹:${NC}"
    echo "â€¢ ğŸš« å®Œå…¨ä¸ä¾èµ–Dockerå’Œå¤–éƒ¨é•œåƒ"
    echo "â€¢ ğŸ”§ ä½¿ç”¨çº¯Pythonå®ç°æ‰€æœ‰æœåŠ¡"  
    echo "â€¢ ğŸ¤– æ”¯æŒå¤šç§AIæ¨¡å‹æ¨ç†"
    echo "â€¢ ğŸ“Š å†…ç½®ç³»ç»Ÿç›‘æ§é¢æ¿"
    echo "â€¢ ğŸ—„ï¸ è½»é‡çº§æ•°æ®åº“å’Œç¼“å­˜"
    echo "â€¢ ğŸŒ æ ‡å‡†HTTP APIæ¥å£"
    
    echo -e "\n${YELLOW}âš ï¸ æ³¨æ„:${NC}"
    echo "â€¢ è¿™æ˜¯æ¼”ç¤ºç‰ˆæœ¬ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨å®Œæ•´Dockeræ–¹æ¡ˆ"
    echo "â€¢ ç›‘æ§æœåŠ¡éœ€è¦psutilæ¨¡å—ï¼Œä¼šè‡ªåŠ¨å°è¯•å®‰è£…"
    echo "â€¢ æ‰€æœ‰æ•°æ®å­˜å‚¨åœ¨æœ¬åœ°ï¼Œé‡å¯åç¼“å­˜æ•°æ®ä¼šä¸¢å¤±"
    
    echo -e "\n${GREEN}âœ¨ ç°åœ¨å¯ä»¥è¿è¡Œ './start_offline.sh' å¯åŠ¨æœåŠ¡äº†ï¼${NC}"
}

# è¿è¡Œä¸»å‡½æ•°
main "$@"