#!/usr/bin/env python3
"""
ç®€å• ONNX æ¨¡å‹ç”Ÿæˆå™¨
ç”¨äºåˆ›å»ºæ¼”ç¤ºç”¨çš„ ONNX æ¨¡å‹
"""

import numpy as np
import onnx
from onnx import helper, TensorProto, mapping
import os


def create_simple_linear_model():
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹"""
    
    # å®šä¹‰è¾“å…¥
    input_tensor = helper.make_tensor_value_info(
        'input', TensorProto.FLOAT, [1, 10]
    )
    
    # å®šä¹‰è¾“å‡º
    output_tensor = helper.make_tensor_value_info(
        'output', TensorProto.FLOAT, [1, 5]
    )
    
    # åˆ›å»ºæƒé‡å¼ é‡
    weight_np = np.random.randn(10, 5).astype(np.float32)
    weight_tensor = helper.make_tensor(
        name='weight',
        data_type=TensorProto.FLOAT,
        dims=weight_np.shape,
        vals=weight_np.flatten()
    )
    
    # åˆ›å»ºåç½®å¼ é‡
    bias_np = np.random.randn(5).astype(np.float32)
    bias_tensor = helper.make_tensor(
        name='bias',
        data_type=TensorProto.FLOAT,
        dims=bias_np.shape,
        vals=bias_np.flatten()
    )
    
    # åˆ›å»º MatMul èŠ‚ç‚¹
    matmul_node = helper.make_node(
        'MatMul',
        inputs=['input', 'weight'],
        outputs=['matmul_output'],
        name='matmul'
    )
    
    # åˆ›å»º Add èŠ‚ç‚¹
    add_node = helper.make_node(
        'Add',
        inputs=['matmul_output', 'bias'],
        outputs=['output'],
        name='add'
    )
    
    # åˆ›å»ºå›¾
    graph = helper.make_graph(
        nodes=[matmul_node, add_node],
        name='SimpleLinearModel',
        inputs=[input_tensor],
        outputs=[output_tensor],
        initializer=[weight_tensor, bias_tensor]
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = helper.make_model(graph, producer_name='simple-model-generator')
    model.opset_import[0].version = 11
    
    return model


def create_simple_cnn_model():
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„ CNN æ¨¡å‹ (ç”¨äºå›¾åƒåˆ†ç±»æ¼”ç¤º)"""
    
    # å®šä¹‰è¾“å…¥ (batch_size, channels, height, width)
    input_tensor = helper.make_tensor_value_info(
        'input', TensorProto.FLOAT, [1, 3, 224, 224]
    )
    
    # å®šä¹‰è¾“å‡º (batch_size, num_classes)
    output_tensor = helper.make_tensor_value_info(
        'output', TensorProto.FLOAT, [1, 1000]
    )
    
    nodes = []
    initializers = []
    
    # å…¨å±€å¹³å‡æ± åŒ–
    gap_node = helper.make_node(
        'GlobalAveragePool',
        inputs=['input'],
        outputs=['gap_output'],
        name='global_average_pool'
    )
    nodes.append(gap_node)
    
    # Flatten
    flatten_node = helper.make_node(
        'Flatten',
        inputs=['gap_output'],
        outputs=['flatten_output'],
        name='flatten',
        axis=1
    )
    nodes.append(flatten_node)
    
    # å…¨è¿æ¥å±‚æƒé‡
    fc_weight = np.random.randn(3, 1000).astype(np.float32) * 0.01
    fc_weight_tensor = helper.make_tensor(
        name='fc_weight',
        data_type=TensorProto.FLOAT,
        dims=fc_weight.shape,
        vals=fc_weight.flatten()
    )
    initializers.append(fc_weight_tensor)
    
    # å…¨è¿æ¥å±‚åç½®
    fc_bias = np.zeros(1000).astype(np.float32)
    fc_bias_tensor = helper.make_tensor(
        name='fc_bias',
        data_type=TensorProto.FLOAT,
        dims=fc_bias.shape,
        vals=fc_bias.flatten()
    )
    initializers.append(fc_bias_tensor)
    
    # å…¨è¿æ¥å±‚
    fc_node = helper.make_node(
        'MatMul',
        inputs=['flatten_output', 'fc_weight'],
        outputs=['fc_output'],
        name='fc'
    )
    nodes.append(fc_node)
    
    # æ·»åŠ åç½®
    add_node = helper.make_node(
        'Add',
        inputs=['fc_output', 'fc_bias'],
        outputs=['output'],
        name='add_bias'
    )
    nodes.append(add_node)
    
    # åˆ›å»ºå›¾
    graph = helper.make_graph(
        nodes=nodes,
        name='SimpleCNNModel',
        inputs=[input_tensor],
        outputs=[output_tensor],
        initializer=initializers
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = helper.make_model(graph, producer_name='simple-cnn-generator')
    model.opset_import[0].version = 11
    
    return model


def save_model(model, filename):
    """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶"""
    # æ£€æŸ¥æ¨¡å‹
    onnx.checker.check_model(model)
    
    # ä¿å­˜æ¨¡å‹
    onnx.save(model, filename)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {filename}")
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    print(f"   æ¨¡å‹å¤§å°: {os.path.getsize(filename) / 1024:.1f} KB")
    
    # æ˜¾ç¤ºè¾“å…¥è¾“å‡ºä¿¡æ¯
    print("   è¾“å…¥:")
    for input_tensor in model.graph.input:
        shape = [dim.dim_value for dim in input_tensor.type.tensor_type.shape.dim]
        print(f"     {input_tensor.name}: {shape}")
    
    print("   è¾“å‡º:")
    for output_tensor in model.graph.output:
        shape = [dim.dim_value for dim in output_tensor.type.tensor_type.shape.dim]
        print(f"     {output_tensor.name}: {shape}")


def main():
    print("ğŸ¤– ONNX æ¨¡å‹ç”Ÿæˆå™¨")
    print("="*50)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs("model-repository/simple-linear-model/1", exist_ok=True)
    os.makedirs("model-repository/simple-cnn-model/1", exist_ok=True)
    
    # ç”Ÿæˆç®€å•çº¿æ€§æ¨¡å‹
    print("\nğŸ“Š ç”Ÿæˆç®€å•çº¿æ€§æ¨¡å‹...")
    linear_model = create_simple_linear_model()
    save_model(linear_model, "model-repository/simple-linear-model/1/model.onnx")
    
    # ç”Ÿæˆçº¿æ€§æ¨¡å‹é…ç½®
    linear_config = '''name: "simple-linear-model"
platform: "onnxruntime_onnx"
max_batch_size: 16

input [
  {
    name: "input"
    data_type: TYPE_FP32
    dims: [ 10 ]
  }
]

output [
  {
    name: "output"
    data_type: TYPE_FP32
    dims: [ 5 ]
  }
]

instance_group [
  {
    count: 1
    kind: KIND_CPU
  }
]

parameters: {
  key: "execution_providers"
  value: { string_value: "CPUExecutionProvider" }
}

dynamic_batching {
  max_queue_delay_microseconds: 100
}'''
    
    with open("model-repository/simple-linear-model/config.pbtxt", "w") as f:
        f.write(linear_config)
    
    # ç”Ÿæˆç®€å• CNN æ¨¡å‹
    print("\nğŸ–¼ï¸ ç”Ÿæˆç®€å• CNN æ¨¡å‹...")
    cnn_model = create_simple_cnn_model()
    save_model(cnn_model, "model-repository/simple-cnn-model/1/model.onnx")
    
    # ç”Ÿæˆ CNN æ¨¡å‹é…ç½®
    cnn_config = '''name: "simple-cnn-model"
platform: "onnxruntime_onnx"
max_batch_size: 4

input [
  {
    name: "input"
    data_type: TYPE_FP32
    dims: [ 3, 224, 224 ]
  }
]

output [
  {
    name: "output"
    data_type: TYPE_FP32
    dims: [ 1000 ]
  }
]

instance_group [
  {
    count: 1
    kind: KIND_CPU
  }
]

parameters: {
  key: "execution_providers"
  value: { string_value: "CPUExecutionProvider" }
}

dynamic_batching {
  max_queue_delay_microseconds: 100
}'''
    
    with open("model-repository/simple-cnn-model/config.pbtxt", "w") as f:
        f.write(cnn_config)
    
    print("\nğŸ‰ æ¨¡å‹ç”Ÿæˆå®Œæˆï¼")
    print("\nğŸ“ æ¨¡å‹æ–‡ä»¶:")
    print("  - model-repository/simple-linear-model/1/model.onnx")
    print("  - model-repository/simple-cnn-model/1/model.onnx")
    
    print("\nğŸ§ª æµ‹è¯•å»ºè®®:")
    print("  1. å¯åŠ¨ Triton æœåŠ¡: docker compose up -d triton-server")
    print("  2. éªŒè¯æ¨¡å‹åŠ è½½: python manage_triton_models.py list-triton")
    print("  3. è¿è¡Œæ¨ç†æµ‹è¯•: python test_triton_client.py")


if __name__ == "__main__":
    try:
        import onnx
        main()
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… onnx åº“: pip install onnx")
        print("   æˆ–è€…ç›´æ¥ä½¿ç”¨é¢„ç”Ÿæˆçš„ç¤ºä¾‹æ¨¡å‹")
