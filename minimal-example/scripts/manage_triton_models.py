#!/usr/bin/env python3
"""
Triton æ¨¡å‹ç®¡ç†å·¥å…·
ç”¨äºç®¡ç† Triton Inference Server çš„æ¨¡å‹ä»“åº“
"""

import os
import json
import shutil
import argparse
import requests
from pathlib import Path
from typing import Dict, List, Optional


class TritonModelManager:
    """Triton æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self, model_repo_path: str = "./model-repository", 
                 triton_url: str = "http://localhost:8100"):
        self.model_repo_path = Path(model_repo_path)
        self.triton_url = triton_url.rstrip('/')
        
    def create_model_directory(self, model_name: str, version: str = "1") -> Path:
        """åˆ›å»ºæ¨¡å‹ç›®å½•ç»“æ„"""
        model_path = self.model_repo_path / model_name
        version_path = model_path / version
        
        model_path.mkdir(parents=True, exist_ok=True)
        version_path.mkdir(parents=True, exist_ok=True)
        
        return model_path
    
    def create_python_model_template(self, model_name: str, 
                                   input_name: str = "INPUT",
                                   output_name: str = "OUTPUT",
                                   input_shape: List[int] = [-1],
                                   output_shape: List[int] = [-1],
                                   datatype: str = "FP32",
                                   max_batch_size: int = 16) -> None:
        """åˆ›å»º Python æ¨¡å‹æ¨¡æ¿"""
        
        model_path = self.create_model_directory(model_name)
        
        # åˆ›å»º config.pbtxt
        config_content = f'''name: "{model_name}"
backend: "python"
max_batch_size: {max_batch_size}

input [
  {{
    name: "{input_name}"
    data_type: TYPE_{datatype}
    dims: {input_shape}
  }}
]

output [
  {{
    name: "{output_name}"
    data_type: TYPE_{datatype}
    dims: {output_shape}
  }}
]

instance_group [
  {{
    count: 1
    kind: KIND_CPU
  }}
]

dynamic_batching {{
  max_queue_delay_microseconds: 100
}}'''
        
        with open(model_path / "config.pbtxt", "w") as f:
            f.write(config_content)
        
        # åˆ›å»º model.py æ¨¡æ¿
        model_py_content = f'''import json
import numpy as np
import triton_python_backend_utils as pb_utils


class TritonPythonModel:
    """
    {model_name} æ¨¡å‹å®ç°
    """

    def initialize(self, args):
        """æ¨¡å‹åˆå§‹åŒ–"""
        self.model_config = model_config = json.loads(args['model_config'])
        
        output_config = pb_utils.get_output_config_by_name(
            model_config, "{output_name}")
        self.output_dtype = pb_utils.triton_string_to_numpy(
            output_config['data_type'])

    def execute(self, requests):
        """æ‰§è¡Œæ¨ç†"""
        responses = []

        for request in requests:
            # è·å–è¾“å…¥
            input_tensor = pb_utils.get_input_tensor_by_name(request, "{input_name}")
            input_data = input_tensor.as_numpy()
            
            # TODO: åœ¨è¿™é‡Œå®ç°æ‚¨çš„æ¨¡å‹é€»è¾‘
            # å½“å‰ç¤ºä¾‹ï¼šç®€å•çš„æ’ç­‰å˜æ¢
            output_data = input_data.astype(self.output_dtype)
            
            # åˆ›å»ºè¾“å‡º
            output_tensor = pb_utils.Tensor("{output_name}", output_data)
            inference_response = pb_utils.InferenceResponse(
                output_tensors=[output_tensor])
            responses.append(inference_response)

        return responses

    def finalize(self):
        """æ¨¡å‹æ¸…ç†"""
        print(f'{model_name} æ¨¡å‹æ¸…ç†å®Œæˆ')'''
        
        with open(model_path / "1" / "model.py", "w") as f:
            f.write(model_py_content)
        
        print(f"âœ… Python æ¨¡å‹æ¨¡æ¿å·²åˆ›å»º: {model_path}")
    
    def create_onnx_model_template(self, model_name: str,
                                 onnx_file_path: str,
                                 input_name: str = "input",
                                 output_name: str = "output",
                                 input_shape: List[int] = [-1, 3, 224, 224],
                                 output_shape: List[int] = [-1, 1000],
                                 datatype: str = "FP32",
                                 max_batch_size: int = 8) -> None:
        """åˆ›å»º ONNX æ¨¡å‹é…ç½®"""
        
        model_path = self.create_model_directory(model_name)
        
        # å¤åˆ¶ ONNX æ–‡ä»¶
        if os.path.exists(onnx_file_path):
            shutil.copy2(onnx_file_path, model_path / "1" / "model.onnx")
        else:
            print(f"âš ï¸ ONNX æ–‡ä»¶ä¸å­˜åœ¨: {onnx_file_path}")
        
        # åˆ›å»º config.pbtxt
        config_content = f'''name: "{model_name}"
platform: "onnxruntime_onnx"
max_batch_size: {max_batch_size}

input [
  {{
    name: "{input_name}"
    data_type: TYPE_{datatype}
    dims: {input_shape}
  }}
]

output [
  {{
    name: "{output_name}"
    data_type: TYPE_{datatype}
    dims: {output_shape}
  }}
]

instance_group [
  {{
    count: 1
    kind: KIND_GPU
  }}
]

optimization {{
  execution_accelerators {{
    gpu_execution_accelerator : [
      {{
        name : "tensorrt"
        parameters {{ key: "precision_mode" value: "FP16" }}
        parameters {{ key: "max_workspace_size_bytes" value: "1073741824" }}
      }}
    ]
  }}
}}'''
        
        with open(model_path / "config.pbtxt", "w") as f:
            f.write(config_content)
        
        print(f"âœ… ONNX æ¨¡å‹é…ç½®å·²åˆ›å»º: {model_path}")
    
    def list_models(self) -> List[str]:
        """åˆ—å‡ºæœ¬åœ°æ¨¡å‹"""
        if not self.model_repo_path.exists():
            return []
        
        models = []
        for item in self.model_repo_path.iterdir():
            if item.is_dir() and (item / "config.pbtxt").exists():
                models.append(item.name)
        
        return models
    
    def validate_model(self, model_name: str) -> bool:
        """éªŒè¯æ¨¡å‹é…ç½®"""
        model_path = self.model_repo_path / model_name
        
        if not model_path.exists():
            print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_path}")
            return False
        
        config_file = model_path / "config.pbtxt"
        if not config_file.exists():
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
            return False
        
        # æ£€æŸ¥ç‰ˆæœ¬ç›®å½•
        version_dirs = [d for d in model_path.iterdir() 
                       if d.is_dir() and d.name.isdigit()]
        if not version_dirs:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°ç‰ˆæœ¬ç›®å½•")
            return False
        
        print(f"âœ… æ¨¡å‹ {model_name} é…ç½®æ­£ç¡®")
        return True
    
    def get_triton_models(self) -> Optional[List[Dict]]:
        """è·å– Triton æœåŠ¡å™¨ä¸Šçš„æ¨¡å‹åˆ—è¡¨"""
        try:
            response = requests.get(f"{self.triton_url}/v2/models")
            if response.status_code == 200:
                return response.json()
            else:
                print(f"âŒ è·å– Triton æ¨¡å‹åˆ—è¡¨å¤±è´¥: {response.status_code}")
                return None
        except Exception as e:
            print(f"âŒ è¿æ¥ Triton æœåŠ¡å™¨å¤±è´¥: {e}")
            return None
    
    def load_model(self, model_name: str) -> bool:
        """åŠ è½½æ¨¡å‹åˆ° Triton"""
        try:
            response = requests.post(f"{self.triton_url}/v2/repository/models/{model_name}/load")
            if response.status_code == 200:
                print(f"âœ… æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ")
                return True
            else:
                print(f"âŒ æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥: {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def unload_model(self, model_name: str) -> bool:
        """ä» Triton å¸è½½æ¨¡å‹"""
        try:
            response = requests.post(f"{self.triton_url}/v2/repository/models/{model_name}/unload")
            if response.status_code == 200:
                print(f"âœ… æ¨¡å‹ {model_name} å¸è½½æˆåŠŸ")
                return True
            else:
                print(f"âŒ æ¨¡å‹ {model_name} å¸è½½å¤±è´¥: {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ å¸è½½æ¨¡å‹å¤±è´¥: {e}")
            return False


def main():
    parser = argparse.ArgumentParser(description="Triton æ¨¡å‹ç®¡ç†å·¥å…·")
    parser.add_argument("--repo-path", default="./model-repository", 
                       help="æ¨¡å‹ä»“åº“è·¯å¾„")
    parser.add_argument("--triton-url", default="http://localhost:8100",
                       help="Triton æœåŠ¡å™¨ URL")
    
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # åˆ—å‡ºæ¨¡å‹
    subparsers.add_parser("list", help="åˆ—å‡ºæœ¬åœ°æ¨¡å‹")
    
    # åˆ—å‡º Triton ä¸Šçš„æ¨¡å‹
    subparsers.add_parser("list-triton", help="åˆ—å‡º Triton æœåŠ¡å™¨ä¸Šçš„æ¨¡å‹")
    
    # åˆ›å»º Python æ¨¡å‹
    create_python_parser = subparsers.add_parser("create-python", help="åˆ›å»º Python æ¨¡å‹æ¨¡æ¿")
    create_python_parser.add_argument("model_name", help="æ¨¡å‹åç§°")
    create_python_parser.add_argument("--input-name", default="INPUT", help="è¾“å…¥å¼ é‡åç§°")
    create_python_parser.add_argument("--output-name", default="OUTPUT", help="è¾“å‡ºå¼ é‡åç§°")
    create_python_parser.add_argument("--max-batch-size", type=int, default=16, help="æœ€å¤§æ‰¹å¤„ç†å¤§å°")
    
    # åˆ›å»º ONNX æ¨¡å‹
    create_onnx_parser = subparsers.add_parser("create-onnx", help="åˆ›å»º ONNX æ¨¡å‹é…ç½®")
    create_onnx_parser.add_argument("model_name", help="æ¨¡å‹åç§°")
    create_onnx_parser.add_argument("onnx_file", help="ONNX æ–‡ä»¶è·¯å¾„")
    create_onnx_parser.add_argument("--input-name", default="input", help="è¾“å…¥å¼ é‡åç§°")
    create_onnx_parser.add_argument("--output-name", default="output", help="è¾“å‡ºå¼ é‡åç§°")
    create_onnx_parser.add_argument("--max-batch-size", type=int, default=8, help="æœ€å¤§æ‰¹å¤„ç†å¤§å°")
    
    # éªŒè¯æ¨¡å‹
    validate_parser = subparsers.add_parser("validate", help="éªŒè¯æ¨¡å‹é…ç½®")
    validate_parser.add_argument("model_name", help="æ¨¡å‹åç§°")
    
    # åŠ è½½æ¨¡å‹
    load_parser = subparsers.add_parser("load", help="åŠ è½½æ¨¡å‹åˆ° Triton")
    load_parser.add_argument("model_name", help="æ¨¡å‹åç§°")
    
    # å¸è½½æ¨¡å‹
    unload_parser = subparsers.add_parser("unload", help="ä» Triton å¸è½½æ¨¡å‹")
    unload_parser.add_argument("model_name", help="æ¨¡å‹åç§°")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    manager = TritonModelManager(args.repo_path, args.triton_url)
    
    if args.command == "list":
        models = manager.list_models()
        if models:
            print("æœ¬åœ°æ¨¡å‹:")
            for model in models:
                print(f"  ğŸ“¦ {model}")
        else:
            print("æ²¡æœ‰æ‰¾åˆ°æœ¬åœ°æ¨¡å‹")
    
    elif args.command == "list-triton":
        models = manager.get_triton_models()
        if models:
            print("Triton æœåŠ¡å™¨ä¸Šçš„æ¨¡å‹:")
            for model in models:
                print(f"  ğŸ”¥ {model['name']} (ç‰ˆæœ¬: {model.get('versions', 'N/A')})")
        else:
            print("æ— æ³•è·å– Triton æœåŠ¡å™¨ä¸Šçš„æ¨¡å‹")
    
    elif args.command == "create-python":
        manager.create_python_model_template(
            args.model_name,
            args.input_name,
            args.output_name,
            max_batch_size=args.max_batch_size
        )
    
    elif args.command == "create-onnx":
        manager.create_onnx_model_template(
            args.model_name,
            args.onnx_file,
            args.input_name,
            args.output_name,
            max_batch_size=args.max_batch_size
        )
    
    elif args.command == "validate":
        manager.validate_model(args.model_name)
    
    elif args.command == "load":
        manager.load_model(args.model_name)
    
    elif args.command == "unload":
        manager.unload_model(args.model_name)


if __name__ == "__main__":
    main()
