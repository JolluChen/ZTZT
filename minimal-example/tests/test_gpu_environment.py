#!/usr/bin/env python3
"""
GPU ç¯å¢ƒæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯ NVIDIA GPU ç¯å¢ƒé…ç½®æ˜¯å¦æ­£ç¡®
"""

import subprocess
import sys
import json
import requests
import time
from typing import Dict, List, Optional


def run_command(cmd: str) -> tuple[int, str]:
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    try:
        result = subprocess.run(cmd.split(), capture_output=True, text=True, timeout=30)
        return result.returncode, result.stdout + result.stderr
    except subprocess.TimeoutExpired:
        return 1, "Command timeout"
    except Exception as e:
        return 1, str(e)


def check_nvidia_driver():
    """æ£€æŸ¥ NVIDIA é©±åŠ¨"""
    print("ğŸ” æ£€æŸ¥ NVIDIA é©±åŠ¨...")
    
    returncode, output = run_command("nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv,noheader,nounits")
    
    if returncode == 0:
        print("âœ… NVIDIA é©±åŠ¨æ­£å¸¸")
        lines = output.strip().split('\n')
        for i, line in enumerate(lines):
            if line.strip():
                name, driver, memory = line.split(', ')
                print(f"   GPU {i}: {name} (é©±åŠ¨: {driver}, æ˜¾å­˜: {memory}MB)")
        return True
    else:
        print("âŒ NVIDIA é©±åŠ¨ä¸å¯ç”¨")
        print(f"   é”™è¯¯: {output}")
        return False


def check_docker_gpu():
    """æ£€æŸ¥ Docker GPU æ”¯æŒ"""
    print("\nğŸ” æ£€æŸ¥ Docker GPU æ”¯æŒ...")
    
    # æ£€æŸ¥ Docker ä¿¡æ¯
    returncode, output = run_command("docker info")
    if returncode != 0:
        print("âŒ Docker ä¸å¯ç”¨")
        return False
    
    if "nvidia" in output.lower():
        print("âœ… Docker æ”¯æŒ NVIDIA GPU")
        
        # æµ‹è¯• GPU å®¹å™¨
        print("   æµ‹è¯• GPU å®¹å™¨...")
        returncode, output = run_command("docker run --rm --gpus all nvidia/cuda:12.0-base-ubuntu22.04 nvidia-smi -L")
        
        if returncode == 0:
            print("âœ… GPU å®¹å™¨æµ‹è¯•æˆåŠŸ")
            gpu_lines = [line for line in output.split('\n') if 'GPU' in line]
            for line in gpu_lines:
                print(f"   {line.strip()}")
            return True
        else:
            print("âŒ GPU å®¹å™¨æµ‹è¯•å¤±è´¥")
            print(f"   é”™è¯¯: {output}")
            return False
    else:
        print("âŒ Docker ä¸æ”¯æŒ NVIDIA GPU")
        print("   è¯·å®‰è£… NVIDIA Container Toolkit")
        return False


def check_triton_service():
    """æ£€æŸ¥ Triton æœåŠ¡"""
    print("\nğŸ” æ£€æŸ¥ Triton Inference Server...")
    
    triton_url = "http://localhost:8100"
    
    try:
        # æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
        response = requests.get(f"{triton_url}/v2/health/ready", timeout=10)
        
        if response.status_code == 200:
            print("âœ… Triton æœåŠ¡æ­£å¸¸è¿è¡Œ")
            
            # è·å–æœåŠ¡å™¨ä¿¡æ¯
            try:
                info_response = requests.get(f"{triton_url}/v2", timeout=5)
                if info_response.status_code == 200:
                    info = info_response.json()
                    print(f"   ç‰ˆæœ¬: {info.get('version', 'Unknown')}")
                    print(f"   æ‰©å±•: {', '.join(info.get('extensions', []))}")
            except Exception:
                pass
            
            # è·å–æ¨¡å‹åˆ—è¡¨
            try:
                models_response = requests.get(f"{triton_url}/v2/models", timeout=5)
                if models_response.status_code == 200:
                    models = models_response.json()
                    print(f"   å·²åŠ è½½æ¨¡å‹æ•°é‡: {len(models)}")
                    for model in models[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ¨¡å‹
                        print(f"   - {model['name']} (ç‰ˆæœ¬: {model.get('versions', 'N/A')})")
            except Exception:
                pass
            
            return True
        else:
            print(f"âŒ Triton æœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
            return False
    
    except requests.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ° Triton æœåŠ¡")
        print("   è¯·ç¡®ä¿ Triton æœåŠ¡æ­£åœ¨è¿è¡Œ: docker compose up -d triton-server")
        return False
    except Exception as e:
        print(f"âŒ Triton æœåŠ¡æ£€æŸ¥å¤±è´¥: {e}")
        return False


def check_ollama_service():
    """æ£€æŸ¥ Ollama æœåŠ¡"""
    print("\nğŸ” æ£€æŸ¥ Ollama æœåŠ¡...")
    
    ollama_url = "http://localhost:11434"
    
    try:
        # æ£€æŸ¥æœåŠ¡çŠ¶æ€
        response = requests.get(f"{ollama_url}/api/tags", timeout=10)
        
        if response.status_code == 200:
            print("âœ… Ollama æœåŠ¡æ­£å¸¸è¿è¡Œ")
            
            models = response.json().get('models', [])
            print(f"   å·²å®‰è£…æ¨¡å‹æ•°é‡: {len(models)}")
            
            for model in models[:5]:  # æ˜¾ç¤ºå‰5ä¸ªæ¨¡å‹
                name = model.get('name', 'Unknown')
                size = model.get('size', 0)
                size_gb = size / (1024**3) if size > 0 else 0
                print(f"   - {name} ({size_gb:.1f}GB)")
            
            return True
        else:
            print(f"âŒ Ollama æœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
            return False
    
    except requests.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡")
        print("   è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ: docker compose up -d ollama")
        return False
    except Exception as e:
        print(f"âŒ Ollama æœåŠ¡æ£€æŸ¥å¤±è´¥: {e}")
        return False


def check_openwebui_service():
    """æ£€æŸ¥ OpenWebUI æœåŠ¡"""
    print("\nğŸ” æ£€æŸ¥ OpenWebUI æœåŠ¡...")
    
    webui_url = "http://localhost:3001"
    
    try:
        response = requests.get(webui_url, timeout=10)
        
        if response.status_code == 200:
            print("âœ… OpenWebUI æœåŠ¡æ­£å¸¸è¿è¡Œ")
            print(f"   è®¿é—®åœ°å€: {webui_url}")
            return True
        else:
            print(f"âŒ OpenWebUI æœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
            return False
    
    except requests.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ° OpenWebUI æœåŠ¡")
        print("   è¯·ç¡®ä¿ OpenWebUI æœåŠ¡æ­£åœ¨è¿è¡Œ: docker compose up -d open-webui")
        return False
    except Exception as e:
        print(f"âŒ OpenWebUI æœåŠ¡æ£€æŸ¥å¤±è´¥: {e}")
        return False


def check_monitoring_services():
    """æ£€æŸ¥ç›‘æ§æœåŠ¡"""
    print("\nğŸ” æ£€æŸ¥ç›‘æ§æœåŠ¡...")
    
    services = {
        "Prometheus": "http://localhost:9090",
        "Grafana": "http://localhost:3002"
    }
    
    all_ok = True
    
    for service_name, url in services.items():
        try:
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                print(f"âœ… {service_name} æœåŠ¡æ­£å¸¸è¿è¡Œ")
                print(f"   è®¿é—®åœ°å€: {url}")
            else:
                print(f"âŒ {service_name} æœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
                all_ok = False
        except requests.ConnectionError:
            print(f"âŒ æ— æ³•è¿æ¥åˆ° {service_name} æœåŠ¡")
            all_ok = False
        except Exception as e:
            print(f"âŒ {service_name} æœåŠ¡æ£€æŸ¥å¤±è´¥: {e}")
            all_ok = False
    
    return all_ok


def run_simple_triton_test():
    """è¿è¡Œç®€å•çš„ Triton æ¨ç†æµ‹è¯•"""
    print("\nğŸ§ª è¿è¡Œ Triton æ¨ç†æµ‹è¯•...")
    
    try:
        returncode, output = run_command("python test_triton_client.py --server-url http://localhost:8100")
        
        if returncode == 0:
            print("âœ… Triton æ¨ç†æµ‹è¯•æˆåŠŸ")
            # æå–å…³é”®ä¿¡æ¯
            lines = output.split('\n')
            for line in lines:
                if any(keyword in line for keyword in ['æ¨ç†ç”¨æ—¶', 'æ¨ç†ç»“æœæ­£ç¡®', 'æˆåŠŸè¯·æ±‚']):
                    print(f"   {line.strip()}")
            return True
        else:
            print("âŒ Triton æ¨ç†æµ‹è¯•å¤±è´¥")
            print(f"   é”™è¯¯: {output}")
            return False
    
    except Exception as e:
        print(f"âŒ æ¨ç†æµ‹è¯•å¼‚å¸¸: {e}")
        return False


def generate_report(results: Dict[str, bool]):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“Š GPU ç¯å¢ƒæµ‹è¯•æŠ¥å‘Š")
    print("="*60)
    
    total_tests = len(results)
    passed_tests = sum(results.values())
    
    print(f"æ€»æµ‹è¯•é¡¹: {total_tests}")
    print(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
    print(f"å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}")
    print(f"é€šè¿‡ç‡: {passed_tests/total_tests*100:.1f}%")
    
    print("\nè¯¦ç»†ç»“æœ:")
    for test_name, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    print("\nå»ºè®®:")
    if not results.get("NVIDIA é©±åŠ¨", False):
        print("  - è¯·å®‰è£… NVIDIA æ˜¾å¡é©±åŠ¨")
    
    if not results.get("Docker GPU æ”¯æŒ", False):
        print("  - è¯·å®‰è£… NVIDIA Container Toolkit")
        print("    å‚è€ƒ: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html")
    
    if not all(results.get(service, False) for service in ["Triton æœåŠ¡", "Ollama æœåŠ¡", "OpenWebUI æœåŠ¡"]):
        print("  - è¯·å¯åŠ¨ç›¸å…³æœåŠ¡: ./start.sh")
    
    if passed_tests == total_tests:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼AI ä¸­å°å¢å¼ºç‰ˆå·²å‡†å¤‡å°±ç»ªã€‚")
        print("\nğŸš€ å¿«é€Ÿå¼€å§‹:")
        print("  - è®¿é—® OpenWebUI: http://localhost:3001")
        print("  - è®¿é—® Triton æœåŠ¡: http://localhost:8100")
        print("  - è®¿é—® Grafana ç›‘æ§: http://localhost:3002 (admin/admin123)")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ ¹æ®ä¸Šè¿°å»ºè®®è¿›è¡Œä¿®å¤ã€‚")


def main():
    print("ğŸ¤– AI ä¸­å°å¢å¼ºç‰ˆ - GPU ç¯å¢ƒæµ‹è¯•")
    print("="*60)
    
    results = {}
    
    # åŸºç¡€ç¯å¢ƒæ£€æŸ¥
    results["NVIDIA é©±åŠ¨"] = check_nvidia_driver()
    results["Docker GPU æ”¯æŒ"] = check_docker_gpu()
    
    # æœåŠ¡æ£€æŸ¥
    results["Triton æœåŠ¡"] = check_triton_service()
    results["Ollama æœåŠ¡"] = check_ollama_service()
    results["OpenWebUI æœåŠ¡"] = check_openwebui_service()
    results["ç›‘æ§æœåŠ¡"] = check_monitoring_services()
    
    # åŠŸèƒ½æµ‹è¯•
    if results["Triton æœåŠ¡"]:
        results["Triton æ¨ç†æµ‹è¯•"] = run_simple_triton_test()
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_report(results)


if __name__ == "__main__":
    main()
