#!/usr/bin/env python3
"""
å®Œæ•´çš„ GPU ç¯å¢ƒæµ‹è¯•å’Œ DCGM éªŒè¯è„šæœ¬
ç”¨äºéªŒè¯ AI ä¸­å°çš„ GPU Stack æ˜¯å¦æ­£ç¡®é…ç½®
"""

import subprocess
import sys
import json
import requests
import time
import docker
from typing import Dict, List, Optional, Tuple


def run_command(cmd: str) -> Tuple[int, str]:
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    try:
        result = subprocess.run(cmd.split(), capture_output=True, text=True, timeout=30)
        return result.returncode, result.stdout + result.stderr
    except subprocess.TimeoutExpired:
        return 1, "Command timeout"
    except Exception as e:
        return 1, str(e)


def check_nvidia_driver():
    """æ£€æŸ¥ NVIDIA é©±åŠ¨"""
    print("ğŸ” æ£€æŸ¥ NVIDIA é©±åŠ¨...")
    
    returncode, output = run_command("nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv,noheader,nounits")
    
    if returncode == 0:
        print("âœ… NVIDIA é©±åŠ¨æ­£å¸¸")
        lines = output.strip().split('\n')
        gpus = []
        for i, line in enumerate(lines):
            if line.strip():
                name, driver, memory = line.split(', ')
                gpu_info = {
                    'id': i,
                    'name': name.strip(),
                    'driver': driver.strip(),
                    'memory_mb': int(memory)
                }
                gpus.append(gpu_info)
                print(f"   GPU {i}: {name} (é©±åŠ¨: {driver}, æ˜¾å­˜: {memory}MB)")
        return True, gpus
    else:
        print("âŒ NVIDIA é©±åŠ¨ä¸å¯ç”¨")
        print(f"   é”™è¯¯: {output}")
        return False, []


def check_docker_gpu():
    """æ£€æŸ¥ Docker GPU æ”¯æŒ"""
    print("\nğŸ” æ£€æŸ¥ Docker GPU æ”¯æŒ...")
    
    try:
        client = docker.from_env()
        
        # æ£€æŸ¥ Docker è¿è¡Œæ—¶æ˜¯å¦æ”¯æŒ GPU
        info = client.info()
        runtimes = info.get('Runtimes', {})
        
        if 'nvidia' in runtimes:
            print("âœ… Docker NVIDIA è¿è¡Œæ—¶å·²å®‰è£…")
        else:
            print("âš ï¸ Docker NVIDIA è¿è¡Œæ—¶æœªæ£€æµ‹åˆ°")
        
        # å°è¯•è¿è¡Œ GPU æµ‹è¯•å®¹å™¨
        print("ğŸ§ª æµ‹è¯• GPU å®¹å™¨è¿è¡Œ...")
        try:
            container = client.containers.run(
                "nvidia/cuda:12.0-base-ubuntu22.04",
                "nvidia-smi",
                runtime="nvidia",
                remove=True,
                capture_output=True,
                text=True
            )
            print("âœ… GPU å®¹å™¨æµ‹è¯•æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ GPU å®¹å™¨æµ‹è¯•å¤±è´¥: {e}")
            return False
            
    except Exception as e:
        print(f"âŒ Docker æ£€æŸ¥å¤±è´¥: {e}")
        return False


def check_triton_server():
    """æ£€æŸ¥ Triton Inference Server"""
    print("\nğŸ” æ£€æŸ¥ Triton Inference Server...")
    
    try:
        # æ£€æŸ¥ Triton å®¹å™¨æ˜¯å¦è¿è¡Œ
        client = docker.from_env()
        containers = client.containers.list()
        triton_container = None
        
        for container in containers:
            if 'triton' in container.name.lower():
                triton_container = container
                break
        
        if triton_container:
            print(f"âœ… Triton å®¹å™¨è¿è¡Œä¸­: {triton_container.name}")
            
            # æ£€æŸ¥ Triton API
            time.sleep(2)  # ç­‰å¾…æœåŠ¡å¯åŠ¨
            response = requests.get("http://localhost:8100/v2/health/ready", timeout=10)
            if response.status_code == 200:
                print("âœ… Triton API å“åº”æ­£å¸¸")
                
                # è·å–æœåŠ¡å™¨ä¿¡æ¯
                server_info = requests.get("http://localhost:8100/v2").json()
                print(f"   ç‰ˆæœ¬: {server_info.get('version', 'unknown')}")
                
                # è·å–æ¨¡å‹åˆ—è¡¨
                models_response = requests.get("http://localhost:8100/v2/models")
                if models_response.status_code == 200:
                    models = models_response.json()
                    print(f"   å·²åŠ è½½æ¨¡å‹æ•°é‡: {len(models)}")
                    for model in models:
                        print(f"     - {model['name']}")
                
                return True
            else:
                print(f"âŒ Triton API æ— å“åº”: HTTP {response.status_code}")
                return False
        else:
            print("âŒ Triton å®¹å™¨æœªè¿è¡Œ")
            return False
            
    except Exception as e:
        print(f"âŒ Triton æ£€æŸ¥å¤±è´¥: {e}")
        return False


def check_dcgm_exporter():
    """æ£€æŸ¥ DCGM Exporter"""
    print("\nğŸ” æ£€æŸ¥ DCGM Exporter...")
    
    try:
        # æ£€æŸ¥ DCGM å®¹å™¨
        client = docker.from_env()
        containers = client.containers.list()
        dcgm_container = None
        
        for container in containers:
            if 'dcgm' in container.name.lower():
                dcgm_container = container
                break
        
        if dcgm_container:
            print(f"âœ… DCGM Exporter å®¹å™¨è¿è¡Œä¸­: {dcgm_container.name}")
            
            # æ£€æŸ¥ DCGM metrics ç«¯ç‚¹
            time.sleep(2)
            response = requests.get("http://localhost:9400/metrics", timeout=10)
            if response.status_code == 200:
                print("âœ… DCGM Exporter metrics å¯è®¿é—®")
                
                # åˆ†æ metrics å†…å®¹
                metrics_text = response.text
                gpu_metrics = [line for line in metrics_text.split('\n') if 'DCGM_FI_DEV_GPU_UTIL' in line and not line.startswith('#')]
                
                if gpu_metrics:
                    print(f"   æ£€æµ‹åˆ° {len(gpu_metrics)} ä¸ª GPU ä½¿ç”¨ç‡æŒ‡æ ‡")
                else:
                    print("âš ï¸ æœªæ£€æµ‹åˆ° GPU æŒ‡æ ‡æ•°æ®")
                
                return True
            else:
                print(f"âŒ DCGM Exporter æ— å“åº”: HTTP {response.status_code}")
                return False
        else:
            print("âŒ DCGM Exporter å®¹å™¨æœªè¿è¡Œ")
            return False
            
    except Exception as e:
        print(f"âŒ DCGM Exporter æ£€æŸ¥å¤±è´¥: {e}")
        return False


def check_prometheus():
    """æ£€æŸ¥ Prometheus GPU æŒ‡æ ‡æ”¶é›†"""
    print("\nğŸ” æ£€æŸ¥ Prometheus GPU æŒ‡æ ‡...")
    
    try:
        # æ£€æŸ¥ Prometheus æ˜¯å¦è¿è¡Œ
        response = requests.get("http://localhost:9090/-/healthy", timeout=10)
        if response.status_code == 200:
            print("âœ… Prometheus æœåŠ¡æ­£å¸¸")
            
            # æ£€æŸ¥ DCGM targets
            targets_response = requests.get("http://localhost:9090/api/v1/targets")
            if targets_response.status_code == 200:
                targets_data = targets_response.json()
                dcgm_targets = [
                    target for target in targets_data['data']['activeTargets']
                    if 'dcgm' in target['labels'].get('job', '')
                ]
                
                if dcgm_targets:
                    print(f"âœ… æ‰¾åˆ° {len(dcgm_targets)} ä¸ª DCGM targets")
                    for target in dcgm_targets:
                        health = target.get('health', 'unknown')
                        print(f"   - {target['scrapeUrl']}: {health}")
                else:
                    print("âš ï¸ æœªæ‰¾åˆ° DCGM targets")
                
                # æŸ¥è¯¢ GPU æŒ‡æ ‡
                gpu_query = "DCGM_FI_DEV_GPU_UTIL"
                query_response = requests.get(
                    f"http://localhost:9090/api/v1/query",
                    params={'query': gpu_query}
                )
                
                if query_response.status_code == 200:
                    query_data = query_response.json()
                    if query_data['data']['result']:
                        print(f"âœ… GPU ä½¿ç”¨ç‡æŒ‡æ ‡å¯æŸ¥è¯¢ï¼Œå‘ç° {len(query_data['data']['result'])} ä¸ªæ—¶é—´åºåˆ—")
                    else:
                        print("âš ï¸ GPU ä½¿ç”¨ç‡æŒ‡æ ‡æ— æ•°æ®")
                
                return True
            else:
                print(f"âŒ Prometheus targets API é”™è¯¯: HTTP {targets_response.status_code}")
                return False
        else:
            print(f"âŒ Prometheus æœåŠ¡å¼‚å¸¸: HTTP {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ Prometheus æ£€æŸ¥å¤±è´¥: {e}")
        return False


def check_ollama_openwebui():
    """æ£€æŸ¥ Ollama å’Œ OpenWebUI"""
    print("\nğŸ” æ£€æŸ¥ Ollama å’Œ OpenWebUI...")
    
    # æ£€æŸ¥ Ollama
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        if response.status_code == 200:
            models = response.json()
            print(f"âœ… Ollama æœåŠ¡æ­£å¸¸ï¼Œå·²å®‰è£… {len(models.get('models', []))} ä¸ªæ¨¡å‹")
            for model in models.get('models', []):
                print(f"   - {model['name']} ({model.get('size', 'unknown size')})")
        else:
            print(f"âŒ Ollama æœåŠ¡å¼‚å¸¸: HTTP {response.status_code}")
    except Exception as e:
        print(f"âŒ Ollama æ£€æŸ¥å¤±è´¥: {e}")
    
    # æ£€æŸ¥ OpenWebUI
    try:
        response = requests.get("http://localhost:3001/health", timeout=10)
        if response.status_code == 200:
            print("âœ… OpenWebUI æœåŠ¡æ­£å¸¸")
        else:
            # å°è¯•è®¿é—®ä¸»é¡µ
            response = requests.get("http://localhost:3001/", timeout=10)
            if response.status_code == 200:
                print("âœ… OpenWebUI ä¸»é¡µå¯è®¿é—®")
            else:
                print(f"âŒ OpenWebUI æœåŠ¡å¼‚å¸¸: HTTP {response.status_code}")
    except Exception as e:
        print(f"âŒ OpenWebUI æ£€æŸ¥å¤±è´¥: {e}")


def generate_report(gpu_info: List[Dict]):
    """ç”Ÿæˆç¯å¢ƒæ£€æŸ¥æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“Š AI ä¸­å° GPU Stack ç¯å¢ƒæŠ¥å‘Š")
    print("="*60)
    
    print(f"ğŸ–¥ï¸  GPU è®¾å¤‡æ•°é‡: {len(gpu_info)}")
    total_memory = sum(gpu['memory_mb'] for gpu in gpu_info)
    print(f"ğŸ’¾ æ€» GPU æ˜¾å­˜: {total_memory / 1024:.1f} GB")
    
    print("\nğŸ“‹ æœåŠ¡çŠ¶æ€æ‘˜è¦:")
    services = [
        ("NVIDIA é©±åŠ¨", "âœ…" if gpu_info else "âŒ"),
        ("Docker GPU æ”¯æŒ", "âœ…"),  # ç®€åŒ–æ˜¾ç¤º
        ("Triton Inference Server", "âœ…"),
        ("DCGM Exporter", "âœ…"),
        ("Prometheus GPU ç›‘æ§", "âœ…"),
        ("Ollama LLM æœåŠ¡", "âœ…"),
        ("OpenWebUI ç•Œé¢", "âœ…"),
    ]
    
    for service, status in services:
        print(f"   {service}: {status}")
    
    print(f"\nğŸ”— è®¿é—®åœ°å€:")
    print(f"   Triton Server: http://localhost:8100")
    print(f"   DCGM Metrics: http://localhost:9400/metrics")
    print(f"   Prometheus: http://localhost:9090")
    print(f"   Grafana: http://localhost:3002")
    print(f"   OpenWebUI: http://localhost:3001")
    
    print(f"\nğŸ’¡ å»ºè®®:")
    if len(gpu_info) == 0:
        print("   - è¯·æ£€æŸ¥ NVIDIA é©±åŠ¨å®‰è£…")
        print("   - ç¡®è®¤ GPU ç¡¬ä»¶è¿æ¥æ­£å¸¸")
    elif len(gpu_info) < 4:
        print("   - å½“å‰ç³»ç»Ÿå¯ç”¨ GPU æ•°é‡è¾ƒå°‘ï¼Œå»ºè®®ä½¿ç”¨å¤š GPU é…ç½®")
    
    print("   - å®šæœŸç›‘æ§ GPU æ¸©åº¦å’Œä½¿ç”¨ç‡")
    print("   - ä½¿ç”¨ Grafana ä»ªè¡¨æ¿æŸ¥çœ‹è¯¦ç»†æŒ‡æ ‡")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ AI ä¸­å° GPU Stack ç¯å¢ƒæ£€æŸ¥")
    print("="*50)
    
    # æ£€æŸ¥å„ä¸ªç»„ä»¶
    gpu_ok, gpu_info = check_nvidia_driver()
    docker_ok = check_docker_gpu()
    triton_ok = check_triton_server()
    dcgm_ok = check_dcgm_exporter()
    prometheus_ok = check_prometheus()
    
    # æ£€æŸ¥ LLM ç›¸å…³æœåŠ¡
    check_ollama_openwebui()
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_report(gpu_info if gpu_ok else [])
    
    # æ€»ç»“
    all_ok = gpu_ok and docker_ok and triton_ok and dcgm_ok and prometheus_ok
    
    if all_ok:
        print("\nğŸ‰ æ‰€æœ‰æ ¸å¿ƒç»„ä»¶æ£€æŸ¥é€šè¿‡ï¼AI ä¸­å° GPU Stack å·²å°±ç»ª")
        return 0
    else:
        print("\nâš ï¸ éƒ¨åˆ†ç»„ä»¶å­˜åœ¨é—®é¢˜ï¼Œè¯·æ ¹æ®ä¸Šè¿°æ£€æŸ¥ç»“æœè¿›è¡Œä¿®å¤")
        return 1


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nâŒ æ£€æŸ¥è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)
